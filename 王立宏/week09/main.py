# coding: utf-8

'''
main.pyï¼šä½¿ç”¨è®­ç»ƒæ•°æ®é›†å®ŒæˆNERå®éªŒ
'''

import torch
import torch.optim as optim
from torch.utils.data import DataLoader
import os
import time
import json
from typing import Dict, Any, Optional
import argparse

from config import Config
from device_utils import set_device, print_device_info, optimize_for_device
from loader import NERDataLoader
from model import create_model, BertCRFForNER
from evaluate import evaluate_model, NEREvaluator


class NERTrainer:
    """
    NERæ¨¡å‹è®­ç»ƒå™¨
    è´Ÿè´£æ¨¡å‹è®­ç»ƒã€éªŒè¯å’Œä¿å­˜
    """
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            config: é…ç½®å­—å…¸
        """
        self.config = config
        self.device = set_device()
        self.label2id, self.id2label = self._load_label_mapping()
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(config['model_path'], exist_ok=True)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.data_loader = NERDataLoader(config)
        self.model = create_model(config)
        self.model.to(self.device)
        
        # è®¾ç½®ä¼˜åŒ–å™¨
        self.optimizer = self._setup_optimizer()
        
        # è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = self._setup_scheduler()
        
        # è®­ç»ƒçŠ¶æ€
        self.global_step = 0
        self.best_f1 = 0.0
        self.train_losses = []
        self.val_metrics = []
    
    def _load_label_mapping(self) -> tuple:
        """åŠ è½½æ ‡ç­¾æ˜ å°„"""
        with open(self.config['schema_path'], 'r', encoding='utf-8') as f:
            schema = json.load(f)
        
        label2id = schema
        id2label = {v: k for k, v in schema.items()}
        
        return label2id, id2label
    
    def _setup_optimizer(self) -> optim.Optimizer:
        """
        è®¾ç½®ä¼˜åŒ–å™¨
        
        Returns:
            ä¼˜åŒ–å™¨å®ä¾‹
        """
        if self.config['optimizer'].lower() == 'adam':
            optimizer = optim.Adam(
                self.model.parameters(),
                lr=self.config['learning_rate'],
                weight_decay=1e-5
            )
        elif self.config['optimizer'].lower() == 'adamw':
            optimizer = optim.AdamW(
                self.model.parameters(),
                lr=self.config['learning_rate'],
                weight_decay=1e-5
            )
        else:
            optimizer = optim.SGD(
                self.model.parameters(),
                lr=self.config['learning_rate'],
                momentum=0.9
            )
        
        return optimizer
    
    def _setup_scheduler(self) -> Optional[object]:
        """
        è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨
        
        Returns:
            å­¦ä¹ ç‡è°ƒåº¦å™¨å®ä¾‹
        """
        from transformers import get_linear_schedule_with_warmup
        
        # è·å–è®­ç»ƒæ•°æ®åŠ è½½å™¨è®¡ç®—æ€»æ­¥æ•°
        train_dataloader, _ = self.data_loader.get_dataloaders()
        total_steps = len(train_dataloader) * self.config['epoch']
        warmup_steps = int(0.1 * total_steps)  # 10%çš„çƒ­èº«
        
        scheduler = get_linear_schedule_with_warmup(
            self.optimizer,
            num_warmup_steps=warmup_steps,
            num_training_steps=total_steps
        )
        
        return scheduler
    
    def train_epoch(self, train_dataloader: DataLoader, epoch: int) -> float:
        """
        è®­ç»ƒä¸€ä¸ªepoch
        
        Args:
            train_dataloader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            epoch: å½“å‰epoch
            
        Returns:
            å¹³å‡æŸå¤±
        """
        self.model.train()
        total_loss = 0.0
        num_batches = len(train_dataloader)
        
        print(f"ğŸš€ å¼€å§‹ç¬¬ {epoch + 1}/{self.config['epoch']} è½®è®­ç»ƒ...")
        
        for batch_idx, batch in enumerate(train_dataloader):
            # å°†æ•°æ®ç§»åˆ°è®¾å¤‡
            input_ids = batch['input_ids'].to(self.device)
            attention_mask = batch['attention_mask'].to(self.device)
            token_type_ids = batch['token_type_ids'].to(self.device)
            labels = batch['labels'].to(self.device)
            
            # æ¸…é›¶æ¢¯åº¦
            self.optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            outputs = self.model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                token_type_ids=token_type_ids,
                labels=labels
            )
            
            loss = outputs['loss']
            
            # CRFè¿”å›çš„æ˜¯æ¯ä¸ªæ ·æœ¬çš„lossï¼Œéœ€è¦å–å¹³å‡
            if loss.dim() > 0:
                loss = loss.mean()
            
            # æ£€æŸ¥æ•°å€¼ç¨³å®šæ€§
            if torch.isnan(loss) or torch.isinf(loss):
                print(f"âš ï¸ æ£€æµ‹åˆ°ä¸ç¨³å®šçš„æŸå¤±å€¼: {loss.item()}ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
                continue
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            # æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            # æ›´æ–°å‚æ•°
            self.optimizer.step()
            self.scheduler.step()
            
            # è®°å½•æŸå¤±
            total_loss += loss.item()
            self.global_step += 1
            
            # æ‰“å°è¿›åº¦
            if (batch_idx + 1) % 10 == 0:
                avg_loss = total_loss / (batch_idx + 1)
                current_lr = self.scheduler.get_last_lr()[0]
                print(f"ğŸ“Š Epoch {epoch + 1}, Batch {batch_idx + 1}/{num_batches}, "
                      f"Loss: {loss.item():.4f}, Avg Loss: {avg_loss:.4f}, LR: {current_lr:.2e}")
        
        avg_loss = total_loss / num_batches
        self.train_losses.append(avg_loss)
        
        return avg_loss
    
    def validate(self, val_dataloader: DataLoader) -> Dict[str, float]:
        """
        éªŒè¯æ¨¡å‹
        
        Args:
            val_dataloader: éªŒè¯æ•°æ®åŠ è½½å™¨
            
        Returns:
            éªŒè¯æŒ‡æ ‡
        """
        print("ğŸ” å¼€å§‹éªŒè¯...")
        
        metrics = evaluate_model(self.model, val_dataloader, self.device, self.id2label)
        self.val_metrics.append(metrics)
        
        return metrics
    
    def save_model(self, epoch: int, metrics: Dict[str, float], is_best: bool = False):
        """
        ä¿å­˜æ¨¡å‹
        
        Args:
            epoch: å½“å‰epoch
            metrics: éªŒè¯æŒ‡æ ‡
            is_best: æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
        """
        # ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'global_step': self.global_step,
            'metrics': metrics,
            'config': self.config
        }
        
        # ä¿å­˜æœ€æ–°æ¨¡å‹
        latest_path = os.path.join(self.config['model_path'], 'latest_model.pt')
        torch.save(checkpoint, latest_path)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if is_best:
            best_path = os.path.join(self.config['model_path'], 'best_model.pt')
            torch.save(checkpoint, best_path)
            print(f"ğŸ† ä¿å­˜æœ€ä½³æ¨¡å‹åˆ°: {best_path}")
        
        # ä¿å­˜è®­ç»ƒå†å²
        history = {
            'train_losses': self.train_losses,
            'val_metrics': self.val_metrics
        }
        history_path = os.path.join(self.config['model_path'], 'training_history.json')
        with open(history_path, 'w', encoding='utf-8') as f:
            json.dump(history, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {latest_path}")
    
    def train(self):
        """
        å®Œæ•´çš„è®­ç»ƒæµç¨‹
        """
        print("ğŸ¯ å¼€å§‹è®­ç»ƒBERT+CRF NERæ¨¡å‹...")
        print(f"ğŸ“‹ é…ç½®ä¿¡æ¯: {json.dumps(self.config, ensure_ascii=False, indent=2)}")
        
        # è·å–æ•°æ®åŠ è½½å™¨
        train_dataloader, val_dataloader = self.data_loader.get_dataloaders()
        
        print(f"ğŸ“Š è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_dataloader)}")
        print(f"ğŸ“Š éªŒè¯æ‰¹æ¬¡æ•°: {len(val_dataloader)}")
        print(f"ğŸ·ï¸  æ ‡ç­¾æ•°é‡: {len(self.label2id)}")
        
        start_time = time.time()
        
        try:
            for epoch in range(self.config['epoch']):
                # è®­ç»ƒä¸€ä¸ªepoch
                train_loss = self.train_epoch(train_dataloader, epoch)
                
                # éªŒè¯
                val_metrics = self.validate(val_dataloader)
                current_f1 = val_metrics['overall_f1']
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
                is_best = current_f1 > self.best_f1
                if is_best:
                    self.best_f1 = current_f1
                
                # ä¿å­˜æ¨¡å‹
                self.save_model(epoch, val_metrics, is_best)
                
                # æ‰“å°epochæ€»ç»“
                print(f"\nğŸ“ˆ Epoch {epoch + 1} æ€»ç»“:")
                print(f"ğŸ“‰ è®­ç»ƒæŸå¤±: {train_loss:.4f}")
                print(f"ğŸ¯ éªŒè¯F1: {current_f1:.4f}")
                print(f"ğŸ† æœ€ä½³F1: {self.best_f1:.4f}")
                print("-" * 60)
        
        except KeyboardInterrupt:
            print("\nâš ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        
        except Exception as e:
            print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            raise
        
        finally:
            # è®¡ç®—è®­ç»ƒæ—¶é—´
            training_time = time.time() - start_time
            print(f"\nâ±ï¸  è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {training_time/60:.2f} åˆ†é’Ÿ")
            print(f"ğŸ† æœ€ä½³éªŒè¯F1å€¼: {self.best_f1:.4f}")
    
    def load_model(self, model_path: str):
        """
        åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
        
        Args:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
        """
        if os.path.exists(model_path):
            checkpoint = torch.load(model_path, map_location=self.device)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.global_step = checkpoint['global_step']
            print(f"âœ… æ¨¡å‹å·²ä» {model_path} åŠ è½½")
        else:
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='BERT+CRF NERæ¨¡å‹è®­ç»ƒ')
    
    parser.add_argument('--config', type=str, default=None,
                       help='é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--mode', type=str, choices=['train', 'eval'], default='train',
                       help='è¿è¡Œæ¨¡å¼ï¼štrainï¼ˆè®­ç»ƒï¼‰æˆ– evalï¼ˆè¯„ä¼°ï¼‰')
    parser.add_argument('--model_path', type=str, default=None,
                       help='æ¨¡å‹åŠ è½½è·¯å¾„ï¼ˆè¯„ä¼°æ¨¡å¼ä½¿ç”¨ï¼‰')
    parser.add_argument('--epoch', type=int, default=None,
                       help='è®­ç»ƒè½®æ•°ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰')
    parser.add_argument('--batch_size', type=int, default=None,
                       help='æ‰¹æ¬¡å¤§å°ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰')
    parser.add_argument('--learning_rate', type=float, default=None,
                       help='å­¦ä¹ ç‡ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰')
    
    return parser.parse_args()


def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_arguments()
    
    # æ‰“å°è®¾å¤‡ä¿¡æ¯
    print_device_info()
    
    # ä¼˜åŒ–è®¾å¤‡è®¾ç½®
    device = set_device()
    optimize_for_device(device)
    
    # æ›´æ–°é…ç½®
    config = Config.copy()
    if args.epoch:
        config['epoch'] = args.epoch
    if args.batch_size:
        config['batch_size'] = args.batch_size
    if args.learning_rate:
        config['learning_rate'] = args.learning_rate
    
    # æ ¹æ®æ¨¡å¼æ‰§è¡Œç›¸åº”æ“ä½œ
    if args.mode == 'train':
        # è®­ç»ƒæ¨¡å¼
        trainer = NERTrainer(config)
        trainer.train()
    
    elif args.mode == 'eval':
        # è¯„ä¼°æ¨¡å¼
        if args.model_path is None:
            # ä½¿ç”¨æœ€ä½³æ¨¡å‹
            model_path = os.path.join(config['model_path'], 'best_model.pt')
        else:
            model_path = args.model_path
        
        if not os.path.exists(model_path):
            print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            return
        
        # åˆ›å»ºè®­ç»ƒå™¨å¹¶åŠ è½½æ¨¡å‹
        trainer = NERTrainer(config)
        trainer.load_model(model_path)
        
        # è·å–éªŒè¯æ•°æ®å¹¶è¯„ä¼°
        _, val_dataloader = trainer.data_loader.get_dataloaders()
        metrics = evaluate_model(trainer.model, val_dataloader, device, trainer.id2label)
        
        # ä¿å­˜è¯„ä¼°ç»“æœ
        eval_result_path = os.path.join(config['model_path'], 'evaluation_results.json')
        with open(eval_result_path, 'w', encoding='utf-8') as f:
            json.dump(metrics, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“„ è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {eval_result_path}")


if __name__ == "__main__":
    main()
