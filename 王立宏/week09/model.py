# coding: utf-8

'''
modelï¼šå®šä¹‰æ¨¡å‹ç»“æ„
'''

import torch
import torch.nn as nn
from transformers import BertModel, BertPreTrainedModel
from typing import List, Tuple, Optional


class CRF(nn.Module):
    """
    æ¡ä»¶éšæœºåœº(CRF)å±‚
    ç”¨äºNERåºåˆ—æ ‡æ³¨ä»»åŠ¡ï¼Œå¤„ç†æ ‡ç­¾ä¹‹é—´çš„ä¾èµ–å…³ç³»
    """
    
    def __init__(self, num_tags: int):
        """
        åˆå§‹åŒ–CRFå±‚
        
        Args:
            num_tags: æ ‡ç­¾æ•°é‡
        """
        super(CRF, self).__init__()
        self.num_tags = num_tags
        
        # è½¬ç§»çŸ©é˜µï¼štransitions[i][j] è¡¨ç¤ºä»æ ‡ç­¾jè½¬ç§»åˆ°æ ‡ç­¾içš„åˆ†æ•°
        self.transitions = nn.Parameter(torch.randn(num_tags, num_tags))
        
        # å¼€å§‹å’Œç»“æŸæ ‡ç­¾çš„ç´¢å¼•
        self.start_tag = num_tags  # å¼€å§‹æ ‡ç­¾
        self.end_tag = num_tags + 1   # ç»“æŸæ ‡ç­¾
        
        # æ‰©å±•è½¬ç§»çŸ©é˜µï¼ŒåŒ…å«å¼€å§‹å’Œç»“æŸæ ‡ç­¾
        # å®é™…ä½¿ç”¨æ—¶çŸ©é˜µå¤§å°ä¸º (num_tags + 2) x (num_tags + 2)
        self.transitions_with_start_end = nn.Parameter(
            torch.randn(num_tags + 2, num_tags + 2)
        )
        
        # åˆå§‹åŒ–å‚æ•°
        self.reset_parameters()
    
    def reset_parameters(self):
        """åˆå§‹åŒ–å‚æ•°"""
        nn.init.uniform_(self.transitions, -0.01, 0.01)  # å‡å°åˆå§‹åŒ–èŒƒå›´
        nn.init.uniform_(self.transitions_with_start_end, -0.01, 0.01)
        
        # è®¾ç½®ä¸€äº›çº¦æŸï¼šä»å¼€å§‹æ ‡ç­¾ä¸èƒ½è½¬ç§»åˆ°ç»“æŸæ ‡ç­¾
        self.transitions_with_start_end.data[self.end_tag, self.start_tag] = -10000
        # è®¾ç½®å…¶ä»–çº¦æŸé€»è¾‘...
    
    def _compute_forward(self, emissions: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—å‰å‘ç®—æ³•ï¼ˆåˆ†å‰²å‡½æ•°ï¼‰
        
        Args:
            emissions: å‘å°„æ¦‚ç‡ [batch_size, seq_len, num_tags]
            mask: æ©ç  [batch_size, seq_len]
            
        Returns:
            å¯¹æ•°é…åˆ†å‡½æ•°
        """
        batch_size, seq_len, num_tags = emissions.size()
        
        # åˆå§‹åŒ–å‰å‘å˜é‡
        # ä»å¼€å§‹æ ‡ç­¾åˆ°ç¬¬ä¸€ä¸ªæ—¶é—´æ­¥çš„æ‰€æœ‰æ ‡ç­¾
        forward_var = emissions[:, 0] + self.transitions_with_start_end[self.start_tag, :num_tags]
        
        for i in range(1, seq_len):
            # å½“å‰æ—¶é—´æ­¥çš„å‘å°„æ¦‚ç‡
            emit_score = emissions[:, i].unsqueeze(2).expand(batch_size, num_tags, num_tags)
            
            # è½¬ç§»åˆ†æ•°
            trans_score = self.transitions[:num_tags, :num_tags].unsqueeze(0).expand(batch_size, num_tags, num_tags)
            
            # å‰å‘é€’æ¨
            next_forward_var = forward_var.unsqueeze(1).expand(batch_size, num_tags, num_tags) + trans_score + emit_score
            
            # ä½¿ç”¨log-sum-expæŠ€å·§é¿å…æ•°å€¼æº¢å‡º
            forward_var = torch.logsumexp(next_forward_var, dim=1)
            
            # åº”ç”¨mask
            mask_i = mask[:, i].unsqueeze(1).expand(batch_size, num_tags)
            forward_var = forward_var * mask_i + forward_var * (~mask_i)
        
        # æ·»åŠ åˆ°ç»“æŸæ ‡ç­¾çš„è½¬ç§»
        forward_var = forward_var + self.transitions_with_start_end[:num_tags, self.end_tag]
        
        return torch.logsumexp(forward_var, dim=1)
    
    def _compute_score(self, emissions: torch.Tensor, tags: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—ç»™å®šæ ‡ç­¾åºåˆ—çš„åˆ†æ•°
        
        Args:
            emissions: å‘å°„æ¦‚ç‡ [batch_size, seq_len, num_tags]
            tags: æ ‡ç­¾åºåˆ— [batch_size, seq_len]
            mask: æ©ç  [batch_size, seq_len]
            
        Returns:
            æ ‡ç­¾åºåˆ—çš„åˆ†æ•°
        """
        batch_size, seq_len, num_tags = emissions.size()
        score = torch.zeros(batch_size, device=emissions.device)
        
        # æ·»åŠ ä»å¼€å§‹æ ‡ç­¾åˆ°ç¬¬ä¸€ä¸ªæ ‡ç­¾çš„è½¬ç§»åˆ†æ•°
        score += self.transitions_with_start_end[self.start_tag, tags[:, 0]]
        
        for i in range(seq_len):
            # æ·»åŠ å‘å°„åˆ†æ•°
            score += emissions[torch.arange(batch_size), i, tags[:, i]] * mask[:, i]
            
            if i > 0:
                # æ·»åŠ è½¬ç§»åˆ†æ•°
                score += self.transitions[tags[:, i], tags[:, i-1]] * mask[:, i]
        
        # æ·»åŠ åˆ°æœ€åä¸€ä¸ªæ ‡ç­¾åˆ°ç»“æŸæ ‡ç­¾çš„è½¬ç§»åˆ†æ•°
        last_tag_indices = mask.sum(1) - 1
        last_tags = tags[torch.arange(batch_size), last_tag_indices]
        score += self.transitions_with_start_end[last_tags, self.end_tag]
        
        return score
    
    def forward(self, emissions: torch.Tensor, tags: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±
        
        Args:
            emissions: å‘å°„æ¦‚ç‡ [batch_size, seq_len, num_tags]
            tags: æ ‡ç­¾åºåˆ— [batch_size, seq_len]
            mask: æ©ç  [batch_size, seq_len]
            
        Returns:
            è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±
        """
        forward_score = self._compute_forward(emissions, mask)
        gold_score = self._compute_score(emissions, tags, mask)
        
        return forward_score - gold_score
    
    def decode(self, emissions: torch.Tensor, mask: torch.Tensor) -> List[List[int]]:
        """
        ç»´ç‰¹æ¯”è§£ç ï¼Œæ‰¾åˆ°æœ€ä¼˜æ ‡ç­¾åºåˆ—
        
        Args:
            emissions: å‘å°„æ¦‚ç‡ [batch_size, seq_len, num_tags]
            mask: æ©ç  [batch_size, seq_len]
            
        Returns:
            æœ€ä¼˜æ ‡ç­¾åºåˆ—åˆ—è¡¨
        """
        batch_size, seq_len, num_tags = emissions.size()
        
        # åˆå§‹åŒ–ç»´ç‰¹æ¯”å˜é‡
        viterbi_vars = []
        viterbi_vars.append(emissions[:, 0] + self.transitions_with_start_end[self.start_tag, :num_tags])
        
        # å›æº¯æŒ‡é’ˆ
        backpointers = []
        
        for i in range(1, seq_len):
            # è®¡ç®—å½“å‰æ—¶é—´æ­¥çš„ç»´ç‰¹æ¯”å˜é‡
            emit_score = emissions[:, i].unsqueeze(2).expand(batch_size, num_tags, num_tags)
            trans_score = self.transitions[:num_tags, :num_tags].unsqueeze(0).expand(batch_size, num_tags, num_tags)
            
            next_viterbi_var = viterbi_vars[-1].unsqueeze(1).expand(batch_size, num_tags, num_tags) + trans_score + emit_score
            
            # æ‰¾åˆ°æœ€ä½³è·¯å¾„
            best_values, best_tags = torch.max(next_viterbi_var, dim=1)
            
            viterbi_vars.append(best_values)
            backpointers.append(best_tags)
        
        # æ·»åŠ ç»“æŸè½¬ç§»
        terminal_var = viterbi_vars[-1] + self.transitions_with_start_end[:num_tags, self.end_tag]
        best_values, best_tags = torch.max(terminal_var, dim=1)
        
        # å›æº¯æ‰¾åˆ°æœ€ä¼˜è·¯å¾„
        path_scores = []
        best_paths = []
        
        for batch_idx in range(batch_size):
            path = [best_tags[batch_idx].item()]
            
            # åå‘å›æº¯
            for backpointer in reversed(backpointers):
                path.append(backpointer[batch_idx, path[-1]].item())
            
            # åè½¬è·¯å¾„
            path.reverse()
            
            # æ ¹æ®maskæˆªæ–­è·¯å¾„
            seq_len_actual = mask[batch_idx].sum().item()
            best_paths.append(path[:seq_len_actual])
        
        return best_paths


class BertCRFForNER(BertPreTrainedModel):
    """
    BERT + CRF çš„NERæ¨¡å‹
    ç»“åˆBERTçš„ç‰¹å¾æå–èƒ½åŠ›å’ŒCRFçš„åºåˆ—å»ºæ¨¡èƒ½åŠ›
    """
    
    def __init__(self, config):
        """
        åˆå§‹åŒ–BERT+CRFæ¨¡å‹
        
        Args:
            config: æ¨¡å‹é…ç½®
        """
        super().__init__(config)
        
        # BERTå±‚
        self.bert = BertModel(config)
        
        # dropoutå±‚
        self.dropout = nn.Dropout(config.hidden_dropout_prob)
        
        # åˆ†ç±»å±‚
        self.classifier = nn.Linear(config.hidden_size, config.num_labels)
        
        # CRFå±‚
        self.crf = CRF(config.num_labels)
        
        # åˆå§‹åŒ–æƒé‡
        self.post_init()
    
    def forward(self,
                input_ids: torch.Tensor,
                attention_mask: torch.Tensor,
                token_type_ids: Optional[torch.Tensor] = None,
                labels: Optional[torch.Tensor] = None,
                return_dict: bool = None) -> dict:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            input_ids: è¾“å…¥token IDs [batch_size, seq_len]
            attention_mask: æ³¨æ„åŠ›æ©ç  [batch_size, seq_len]
            token_type_ids: tokenç±»å‹ IDs [batch_size, seq_len]
            labels: æ ‡ç­¾ [batch_size, seq_len]
            return_dict: æ˜¯å¦è¿”å›å­—å…¸æ ¼å¼
            
        Returns:
            åŒ…å«æŸå¤±å’Œé¢„æµ‹ç»“æœçš„å­—å…¸
        """
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
        
        # BERTç‰¹å¾æå–
        outputs = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            return_dict=return_dict
        )
        
        # è·å–åºåˆ—è¡¨ç¤º
        sequence_output = outputs[0]  # [batch_size, seq_len, hidden_size]
        
        # Dropout
        sequence_output = self.dropout(sequence_output)
        
        # åˆ†ç±»å±‚å¾—åˆ°å‘å°„æ¦‚ç‡
        emissions = self.classifier(sequence_output)  # [batch_size, seq_len, num_labels]
        
        # åˆ›å»ºmask (å¿½ç•¥padding tokens)
        mask = attention_mask.bool()
        
        result = {
            'emissions': emissions,
            'mask': mask
        }
        
        if labels is not None:
            # è®¡ç®—CRFæŸå¤±
            # æ³¨æ„ï¼šéœ€è¦å°†-100è½¬æ¢ä¸ºæœ‰æ•ˆçš„mask
            labels_mask = (labels != -100)
            loss = self.crf(emissions, labels, labels_mask)
            result['loss'] = loss
        
        return result
    
    def decode(self, emissions: torch.Tensor, mask: torch.Tensor) -> List[List[int]]:
        """
        è§£ç å¾—åˆ°æœ€ä¼˜æ ‡ç­¾åºåˆ—
        
        Args:
            emissions: å‘å°„æ¦‚ç‡ [batch_size, seq_len, num_labels]
            mask: æ©ç  [batch_size, seq_len]
            
        Returns:
            æœ€ä¼˜æ ‡ç­¾åºåˆ—åˆ—è¡¨
        """
        return self.crf.decode(emissions, mask)


def create_model(config: dict) -> BertCRFForNER:
    """
    åˆ›å»ºNERæ¨¡å‹
    
    Args:
        config: é…ç½®å­—å…¸
        
    Returns:
        åˆå§‹åŒ–çš„æ¨¡å‹
    """
    # ä»é¢„è®­ç»ƒBERTæ¨¡å‹åŠ è½½é…ç½®
    from transformers import BertConfig
    
    bert_config = BertConfig.from_pretrained(config['bert_path'])
    bert_config.num_labels = config['class_num']
    bert_config.hidden_dropout_prob = 0.1
    
    # åˆ›å»ºæ¨¡å‹
    model = BertCRFForNER.from_pretrained(
        config['bert_path'],
        config=bert_config
    )
    
    return model


def test_model():
    """
    æµ‹è¯•æ¨¡å‹åŠŸèƒ½
    """
    from config import Config
    
    print("ğŸ§ª æµ‹è¯•BERT+CRFæ¨¡å‹...")
    
    # åˆ›å»ºæ¨¡å‹
    model = create_model(Config)
    
        # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size, seq_len = Config['batch_size'], Config['max_length']
    input_ids = torch.randint(0, 1000, (batch_size, seq_len), dtype=torch.long)
    attention_mask = torch.ones(batch_size, seq_len, dtype=torch.long)
    token_type_ids = torch.zeros(batch_size, seq_len, dtype=torch.long)
    labels = torch.randint(0, Config['class_num'], (batch_size, seq_len), dtype=torch.long)
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    with torch.no_grad():
        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            labels=labels
        )
        
        print(f"ğŸ“Š Emissionså½¢çŠ¶: {outputs['emissions'].shape}")
        print(f"ğŸ’° Losså€¼: {outputs['loss'].mean().item():.4f}")
        
        # æµ‹è¯•è§£ç 
        predictions = model.decode(outputs['emissions'], outputs['mask'])
        print(f"ğŸ¯ é¢„æµ‹ç»“æœæ•°é‡: {len(predictions)}")
        print(f"ğŸ“ ç¬¬ä¸€ä¸ªé¢„æµ‹é•¿åº¦: {len(predictions[0])}")
    
    print("âœ… æ¨¡å‹æµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    test_model()

