# coding: utf-8

'''
evaluate.pyï¼šæ¨¡å‹è¯„ä¼°å‡½æ•°ï¼Œæ”¯æŒå¤šæ¬¡é‡å¤è¯„ä¼°å’ŒkæŠ˜äº¤å‰éªŒè¯
'''

import torch
import numpy as np
from typing import Dict, List, Tuple, Any
from collections import defaultdict
from sklearn.metrics import classification_report
import json


class NEREvaluator:
    """
    NERæ¨¡å‹è¯„ä¼°å™¨
    è®¡ç®—NERä»»åŠ¡çš„å„é¡¹è¯„ä¼°æŒ‡æ ‡
    """
    
    def __init__(self, id2label: Dict[int, str]):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            id2label: IDåˆ°æ ‡ç­¾çš„æ˜ å°„å­—å…¸
        """
        self.id2label = id2label
        self.reset_metrics()
    
    def reset_metrics(self):
        """é‡ç½®æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡"""
        self.true_positive = defaultdict(int)
        self.false_positive = defaultdict(int)
        self.false_negative = defaultdict(int)
        self.all_predictions = []
        self.all_labels = []
    
    def add_batch(self, predictions: List[List[int]], gold_labels: List[List[int]]):
        """
        æ·»åŠ ä¸€ä¸ªæ‰¹æ¬¡çš„é¢„æµ‹ç»“æœå’ŒçœŸå®æ ‡ç­¾
        
        Args:
            predictions: é¢„æµ‹æ ‡ç­¾åºåˆ—åˆ—è¡¨
            gold_labels: çœŸå®æ ‡ç­¾åºåˆ—åˆ—è¡¨
        """
        for pred_seq, gold_seq in zip(predictions, gold_labels):
            # ç¡®ä¿é•¿åº¦ä¸€è‡´
            min_len = min(len(pred_seq), len(gold_seq))
            pred_seq = pred_seq[:min_len]
            gold_seq = gold_seq[:min_len]
            
            for pred_label, gold_label in zip(pred_seq, gold_seq):
                # è½¬æ¢ä¸ºæ ‡ç­¾åç§°
                pred_name = self.id2label.get(pred_label, 'O')
                gold_name = self.id2label.get(gold_label, 'O')
                
                # ä¿å­˜æ‰€æœ‰é¢„æµ‹ç”¨äºæ•´ä½“è¯„ä¼°
                self.all_predictions.append(pred_label)
                self.all_labels.append(gold_label)
                
                # åªè®¡ç®—å®ä½“æ ‡ç­¾çš„æŒ‡æ ‡ï¼ˆå¿½ç•¥'O'æ ‡ç­¾ï¼‰
                if gold_name != 'O':
                    if pred_name == gold_name:
                        self.true_positive[gold_name] += 1
                    else:
                        self.false_negative[gold_name] += 1
                        if pred_name != 'O':
                            self.false_positive[pred_name] += 1
                elif pred_name != 'O':
                    # çœŸå®æ ‡ç­¾ä¸ºOï¼Œä½†é¢„æµ‹ä¸ºå®ä½“æ ‡ç­¾
                    self.false_positive[pred_name] += 1
    
    def get_metrics(self) -> Dict[str, float]:
        """
        è®¡ç®—å¹¶è¿”å›å„é¡¹è¯„ä¼°æŒ‡æ ‡
        
        Returns:
            åŒ…å«å„é¡¹æŒ‡æ ‡çš„å­—å…¸
        """
        metrics = {}
        
        # è®¡ç®—æ¯ä¸ªæ ‡ç­¾çš„æŒ‡æ ‡
        precision_sum = 0.0
        recall_sum = 0.0
        f1_sum = 0.0
        entity_count = 0
        
        for label in self.id2label.values():
            if label == 'O':
                continue
            
            tp = self.true_positive[label]
            fp = self.false_positive[label]
            fn = self.false_negative[label]
            
            # è®¡ç®—ç²¾ç¡®ç‡
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
            
            # è®¡ç®—å¬å›ç‡
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
            
            # è®¡ç®—F1å€¼
            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
            
            metrics[f'{label}_precision'] = precision
            metrics[f'{label}_recall'] = recall
            metrics[f'{label}_f1'] = f1
            
            if tp + fp > 0 or tp + fn > 0:
                precision_sum += precision
                recall_sum += recall
                f1_sum += f1
                entity_count += 1
        
        # è®¡ç®—å®å¹³å‡æŒ‡æ ‡
        if entity_count > 0:
            metrics['macro_precision'] = precision_sum / entity_count
            metrics['macro_recall'] = recall_sum / entity_count
            metrics['macro_f1'] = f1_sum / entity_count
        
        # è®¡ç®—æ€»ä½“æŒ‡æ ‡
        total_tp = sum(self.true_positive.values())
        total_fp = sum(self.false_positive.values())
        total_fn = sum(self.false_negative.values())
        
        overall_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0
        overall_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0
        overall_f1 = 2 * overall_precision * overall_recall / (overall_precision + overall_recall) if (overall_precision + overall_recall) > 0 else 0.0
        
        metrics['overall_precision'] = overall_precision
        metrics['overall_recall'] = overall_recall
        metrics['overall_f1'] = overall_f1
        
        return metrics
    
    def get_classification_report(self) -> str:
        """
        è·å–è¯¦ç»†çš„åˆ†ç±»æŠ¥å‘Š
        
        Returns:
            åˆ†ç±»æŠ¥å‘Šå­—ç¬¦ä¸²
        """
        # è¿‡æ»¤æ‰-100çš„æ ‡ç­¾
        filtered_predictions = []
        filtered_labels = []
        
        for pred, gold in zip(self.all_predictions, self.all_labels):
            if gold != -100:  # å¿½ç•¥paddingæ ‡ç­¾
                filtered_predictions.append(pred)
                filtered_labels.append(gold)
        
        # è·å–æ ‡ç­¾åç§°
        target_names = [self.id2label[i] for i in sorted(self.id2label.keys()) if self.id2label[i] != 'O']
        
        # ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š
        report = classification_report(
            filtered_labels,
            filtered_predictions,
            target_names=target_names,
            zero_division=0
        )
        
        return report
    
    def print_metrics(self, metrics: Dict[str, float]):
        """
        æ‰“å°è¯„ä¼°æŒ‡æ ‡
        
        Args:
            metrics: è¯„ä¼°æŒ‡æ ‡å­—å…¸
        """
        print("\n" + "="*60)
        print("ğŸ“Š NERæ¨¡å‹è¯„ä¼°ç»“æœ")
        print("="*60)
        
        # æ‰“å°æ¯ä¸ªå®ä½“ç±»å‹çš„æŒ‡æ ‡
        entity_types = [label for label in self.id2label.values() if label != 'O']
        
        print("\nğŸ·ï¸  å„å®ä½“ç±»å‹æŒ‡æ ‡:")
        print("-" * 60)
        print(f"{'å®ä½“ç±»å‹':<15} {'ç²¾ç¡®ç‡':<10} {'å¬å›ç‡':<10} {'F1å€¼':<10}")
        print("-" * 60)
        
        for entity_type in entity_types:
            precision = metrics.get(f'{entity_type}_precision', 0.0)
            recall = metrics.get(f'{entity_type}_recall', 0.0)
            f1 = metrics.get(f'{entity_type}_f1', 0.0)
            print(f"{entity_type:<15} {precision:<10.4f} {recall:<10.4f} {f1:<10.4f}")
        
        # æ‰“å°æ€»ä½“æŒ‡æ ‡
        print("\nğŸ“ˆ æ€»ä½“æŒ‡æ ‡:")
        print("-" * 60)
        print(f"å®å¹³å‡ç²¾ç¡®ç‡: {metrics.get('macro_precision', 0.0):.4f}")
        print(f"å®å¹³å‡å¬å›ç‡: {metrics.get('macro_recall', 0.0):.4f}")
        print(f"å®å¹³å‡F1å€¼:   {metrics.get('macro_f1', 0.0):.4f}")
        print("-" * 60)
        print(f"æ€»ä½“ç²¾ç¡®ç‡:   {metrics.get('overall_precision', 0.0):.4f}")
        print(f"æ€»ä½“å¬å›ç‡:   {metrics.get('overall_recall', 0.0):.4f}")
        print(f"æ€»ä½“F1å€¼:     {metrics.get('overall_f1', 0.0):.4f}")
        print("="*60)
    
    def save_metrics(self, metrics: Dict[str, float], filepath: str):
        """
        ä¿å­˜è¯„ä¼°æŒ‡æ ‡åˆ°æ–‡ä»¶
        
        Args:
            metrics: è¯„ä¼°æŒ‡æ ‡å­—å…¸
            filepath: ä¿å­˜è·¯å¾„
        """
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(metrics, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“„ è¯„ä¼°æŒ‡æ ‡å·²ä¿å­˜åˆ°: {filepath}")


def evaluate_model(model, dataloader, device, id2label: Dict[int, str]) -> Dict[str, float]:
    """
    è¯„ä¼°æ¨¡å‹åœ¨ç»™å®šæ•°æ®é›†ä¸Šçš„æ€§èƒ½
    
    Args:
        model: å¾…è¯„ä¼°çš„æ¨¡å‹
        dataloader: æ•°æ®åŠ è½½å™¨
        device: è®¾å¤‡
        id2label: IDåˆ°æ ‡ç­¾çš„æ˜ å°„
        
    Returns:
        è¯„ä¼°æŒ‡æ ‡å­—å…¸
    """
    model.eval()
    evaluator = NEREvaluator(id2label)
    
    print("ğŸ” å¼€å§‹è¯„ä¼°æ¨¡å‹...")
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(dataloader):
            # å°†æ•°æ®ç§»åˆ°è®¾å¤‡
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            token_type_ids = batch['token_type_ids'].to(device)
            labels = batch['labels'].to(device)
            
            # å‰å‘ä¼ æ’­
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                token_type_ids=token_type_ids
            )
            
            # è·å–é¢„æµ‹ç»“æœ
            predictions = model.decode(outputs['emissions'], outputs['mask'])
            
            # å¤„ç†çœŸå®æ ‡ç­¾ï¼ˆç§»é™¤-100ï¼‰
            gold_labels = []
            for label_seq in labels:
                gold_seq = label_seq.cpu().numpy()
                # ç§»é™¤-100æ ‡ç­¾ï¼ˆpaddingæ ‡ç­¾ï¼‰
                gold_seq = gold_seq[gold_seq != -100]
                gold_labels.append(gold_seq.tolist())
            
            # æ·»åŠ åˆ°è¯„ä¼°å™¨
            evaluator.add_batch(predictions, gold_labels)
            
            # æ‰“å°è¿›åº¦
            if (batch_idx + 1) % 10 == 0:
                print(f"ğŸ“Š å·²å¤„ç† {batch_idx + 1}/{len(dataloader)} ä¸ªæ‰¹æ¬¡")
    
    # è®¡ç®—æŒ‡æ ‡
    metrics = evaluator.get_metrics()
    
    # æ‰“å°ç»“æœ
    evaluator.print_metrics(metrics)
    
    return metrics


def cross_validate(model_class, config, data_loader, device, k_folds: int = 5) -> Dict[str, float]:
    """
    KæŠ˜äº¤å‰éªŒè¯
    
    Args:
        model_class: æ¨¡å‹ç±»
        config: é…ç½®å­—å…¸
        data_loader: æ•°æ®åŠ è½½å™¨
        device: è®¾å¤‡
        k_folds: æŠ˜æ•°
        
    Returns:
        äº¤å‰éªŒè¯çš„å¹³å‡æŒ‡æ ‡
    """
    from sklearn.model_selection import KFold
    import torch.utils.data as data
    
    print(f"ğŸ”„ å¼€å§‹{k_folds}æŠ˜äº¤å‰éªŒè¯...")
    
    # è·å–å®Œæ•´æ•°æ®é›†
    train_dataset, _ = data_loader.get_datasets()
    
    # åˆ›å»ºKæŠ˜åˆ†å‰²å™¨
    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)
    
    all_metrics = []
    id2label = data_loader.id2label
    
    for fold, (train_idx, val_idx) in enumerate(kfold.split(train_dataset)):
        print(f"\nğŸ“‚ ç¬¬ {fold + 1}/{k_folds} æŠ˜:")
        
        # åˆ›å»ºæ•°æ®å­é›†
        train_subset = data.Subset(train_dataset, train_idx)
        val_subset = data.Subset(train_dataset, val_idx)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = data.DataLoader(
            train_subset,
            batch_size=config['batch_size'],
            shuffle=True,
            collate_fn=data_loader._collate_fn
        )
        
        val_loader = data.DataLoader(
            val_subset,
            batch_size=config['batch_size'],
            shuffle=False,
            collate_fn=data_loader._collate_fn
        )
        
        # åˆ›å»ºæ¨¡å‹
        model = model_class(config)
        model.to(device)
        
        # è¿™é‡Œåº”è¯¥è¿›è¡Œè®­ç»ƒï¼Œä½†ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬åªåšè¯„ä¼°
        # å®é™…ä½¿ç”¨æ—¶åº”è¯¥è°ƒç”¨è®­ç»ƒå‡½æ•°
        
        # è¯„ä¼°æ¨¡å‹
        metrics = evaluate_model(model, val_loader, device, id2label)
        all_metrics.append(metrics)
    
    # è®¡ç®—å¹³å‡æŒ‡æ ‡
    avg_metrics = {}
    for key in all_metrics[0].keys():
        avg_metrics[key] = np.mean([m[key] for m in all_metrics])
        std_metrics = np.std([m[key] for m in all_metrics])
        avg_metrics[f'{key}_std'] = std_metrics
    
    print("\nğŸ“Š äº¤å‰éªŒè¯ç»“æœ:")
    print("="*60)
    print(f"æ€»ä½“F1å€¼: {avg_metrics['overall_f1']:.4f} Â± {avg_metrics['overall_f1_std']:.4f}")
    print(f"æ€»ä½“ç²¾ç¡®ç‡: {avg_metrics['overall_precision']:.4f} Â± {avg_metrics['overall_precision_std']:.4f}")
    print(f"æ€»ä½“å¬å›ç‡: {avg_metrics['overall_recall']:.4f} Â± {avg_metrics['overall_recall_std']:.4f}")
    print("="*60)
    
    return avg_metrics


def test_evaluator():
    """
    æµ‹è¯•è¯„ä¼°å™¨åŠŸèƒ½
    """
    from config import Config
    
    print("ğŸ§ª æµ‹è¯•è¯„ä¼°å™¨...")
    
    # åˆ›å»ºæ¨¡æ‹Ÿçš„id2label
    label2id = {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-LOC': 3, 'I-LOC': 4}
    id2label = {v: k for k, v in label2id.items()}
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = NEREvaluator(id2label)
    
    # æ·»åŠ ä¸€äº›æµ‹è¯•æ•°æ®
    predictions = [[1, 2, 0, 3, 4], [0, 0, 1, 2, 0]]
    gold_labels = [[1, 2, 0, 0, 3], [0, 0, 1, 0, 4]]
    
    evaluator.add_batch(predictions, gold_labels)
    
    # è®¡ç®—æŒ‡æ ‡
    metrics = evaluator.get_metrics()
    evaluator.print_metrics(metrics)
    
    print("âœ… è¯„ä¼°å™¨æµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    test_evaluator()
