# coding: utf-8

'''
loader.pyï¼šåŠ è½½æ•°æ®é›†ï¼Œåšé¢„å¤„ç†ï¼Œä¸ºè®­ç»ƒåšå‡†å¤‡
'''

import torch
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizerFast
import json
from typing import List, Tuple, Dict, Any
import numpy as np


class NERDataset(Dataset):
    """
    NERæ•°æ®é›†ç±»
    ç”¨äºå¤„ç†å‘½åå®ä½“è¯†åˆ«ä»»åŠ¡çš„æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
    """
    
    def __init__(self, 
                 data_path: str, 
                 tokenizer: BertTokenizerFast, 
                 label2id: Dict[str, int],
                 max_length: int = 128):
        """
        åˆå§‹åŒ–NERæ•°æ®é›†
        
        Args:
            data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
            tokenizer: BERTåˆ†è¯å™¨
            label2id: æ ‡ç­¾åˆ°IDçš„æ˜ å°„å­—å…¸
            max_length: æœ€å¤§åºåˆ—é•¿åº¦
        """
        self.data_path = data_path
        self.tokenizer = tokenizer
        self.label2id = label2id
        self.max_length = max_length
        self.samples = self._load_data()
    
    def _load_data(self) -> List[Dict[str, Any]]:
        """
        åŠ è½½NERæ•°æ®
        æ•°æ®æ ¼å¼ï¼šæ¯è¡ŒåŒ…å«ä¸€ä¸ªå­—ç¬¦å’Œå¯¹åº”çš„æ ‡ç­¾ï¼Œç”¨ç©ºæ ¼åˆ†éš”
        å¥å­ä¹‹é—´ç”¨ç©ºè¡Œåˆ†éš”
        
        Returns:
            å¤„ç†åçš„æ ·æœ¬åˆ—è¡¨
        """
        samples = []
        with open(self.data_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        current_tokens = []
        current_labels = []
        
        for line in lines:
            line = line.strip()
            if not line:  # ç©ºè¡Œè¡¨ç¤ºå¥å­ç»“æŸ
                if current_tokens:  # ä¿å­˜å½“å‰å¥å­
                    samples.append({
                        'tokens': current_tokens,
                        'labels': current_labels
                    })
                    current_tokens = []
                    current_labels = []
            else:
                try:
                    token, label = line.split()
                    current_tokens.append(token)
                    current_labels.append(label)
                except ValueError:
                    # å¦‚æœåˆ†å‰²å¤±è´¥ï¼Œè·³è¿‡è¯¥è¡Œ
                    continue
        
        # å¤„ç†æœ€åä¸€ä¸ªå¥å­
        if current_tokens:
            samples.append({
                'tokens': current_tokens,
                'labels': current_labels
            })
        
        return samples
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        """
        è·å–å•ä¸ªæ ·æœ¬
        
        Args:
            idx: æ ·æœ¬ç´¢å¼•
            
        Returns:
            åŒ…å«æ¨¡å‹è¾“å…¥çš„å­—å…¸
        """
        sample = self.samples[idx]
        tokens = sample['tokens']
        labels = sample['labels']
        
        # ä½¿ç”¨BERTåˆ†è¯å™¨è¿›è¡Œç¼–ç 
        # æ³¨æ„ï¼šå¯¹äºä¸­æ–‡ï¼Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨å­—ç¬¦çº§åˆ«çš„åˆ†è¯
        encoded = self.tokenizer(
            tokens,
            is_split_into_words=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        # å¤„ç†æ ‡ç­¾å¯¹é½
        word_ids = encoded.word_ids()
        aligned_labels = []
        previous_word_idx = None
        
        for word_idx in word_ids:
            if word_idx is None:
                # [CLS], [SEP], [PAD]ç­‰ç‰¹æ®Štoken
                aligned_labels.append(-100)  # -100ä¼šè¢«CrossEntropyLosså¿½ç•¥
            elif word_idx != previous_word_idx:
                # æ–°è¯çš„å¼€å§‹
                if word_idx < len(labels):
                    aligned_labels.append(self.label2id[labels[word_idx]])
                else:
                    aligned_labels.append(-100)
            else:
                # åŒä¸€ä¸ªè¯çš„åç»­éƒ¨åˆ†ï¼ˆå¯¹äºä¸­æ–‡åŸºæœ¬ä¸ä¼šå‡ºç°ï¼Œå› ä¸ºä¸­æ–‡å­—ç¬¦ä¸ä¼šè¢«æ‹†åˆ†ï¼‰
                aligned_labels.append(-100)
            
            previous_word_idx = word_idx
        
        # è¿”å›æ¨¡å‹è¾“å…¥
        return {
            'input_ids': encoded['input_ids'].squeeze(0),
            'attention_mask': encoded['attention_mask'].squeeze(0),
            'token_type_ids': encoded['token_type_ids'].squeeze(0),
            'labels': torch.tensor(aligned_labels, dtype=torch.long),
            'tokens': tokens,  # ä¿å­˜åŸå§‹tokensç”¨äºè¯„ä¼°
            'original_labels': labels  # ä¿å­˜åŸå§‹labelsç”¨äºè¯„ä¼°
        }


class NERDataLoader:
    """
    NERæ•°æ®åŠ è½½å™¨ç±»
    è´Ÿè´£åˆ›å»ºè®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•çš„æ•°æ®åŠ è½½å™¨
    """
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        
        Args:
            config: é…ç½®å­—å…¸
        """
        self.config = config
        self.tokenizer = BertTokenizerFast.from_pretrained(config['bert_path'])
        self.label2id, self.id2label = self._load_schema()
        
    def _load_schema(self) -> Tuple[Dict[str, int], Dict[int, str]]:
        """
        åŠ è½½æ ‡ç­¾schema
        
        Returns:
            label2id: æ ‡ç­¾åˆ°IDçš„æ˜ å°„
            id2label: IDåˆ°æ ‡ç­¾çš„æ˜ å°„
        """
        with open(self.config['schema_path'], 'r', encoding='utf-8') as f:
            schema = json.load(f)
        
        label2id = schema
        id2label = {v: k for k, v in schema.items()}
        
        return label2id, id2label
    
    def get_datasets(self) -> Tuple[NERDataset, NERDataset]:
        """
        è·å–è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†
        
        Returns:
            train_dataset, valid_dataset
        """
        train_dataset = NERDataset(
            data_path=self.config['train_data_path'],
            tokenizer=self.tokenizer,
            label2id=self.label2id,
            max_length=self.config['max_length']
        )
        
        valid_dataset = NERDataset(
            data_path=self.config['valid_data_path'],
            tokenizer=self.tokenizer,
            label2id=self.label2id,
            max_length=self.config['max_length']
        )
        
        return train_dataset, valid_dataset
    
    def get_dataloaders(self) -> Tuple[DataLoader, DataLoader]:
        """
        è·å–è®­ç»ƒå’ŒéªŒè¯æ•°æ®åŠ è½½å™¨
        
        Returns:
            train_dataloader, valid_dataloader
        """
        train_dataset, valid_dataset = self.get_datasets()
        
        train_dataloader = DataLoader(
            train_dataset,
            batch_size=self.config['batch_size'],
            shuffle=True,
            collate_fn=self._collate_fn
        )
        
        valid_dataloader = DataLoader(
            valid_dataset,
            batch_size=self.config['batch_size'],
            shuffle=False,
            collate_fn=self._collate_fn
        )
        
        return train_dataloader, valid_dataloader
    
    def _collate_fn(self, batch):
        """
        è‡ªå®šä¹‰æ‰¹å¤„ç†å‡½æ•°
        
        Args:
            batch: æ‰¹é‡æ ·æœ¬
            
        Returns:
            å¤„ç†åçš„æ‰¹é‡æ•°æ®
        """
        # å°†batchä¸­çš„æ•°æ®å †å 
        input_ids = torch.stack([item['input_ids'] for item in batch])
        attention_mask = torch.stack([item['attention_mask'] for item in batch])
        token_type_ids = torch.stack([item['token_type_ids'] for item in batch])
        labels = torch.stack([item['labels'] for item in batch])
        
        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'token_type_ids': token_type_ids,
            'labels': labels,
            'tokens': [item['tokens'] for item in batch],
            'original_labels': [item['original_labels'] for item in batch]
        }
    
    def get_label_mappings(self) -> Tuple[Dict[str, int], Dict[int, str]]:
        """
        è·å–æ ‡ç­¾æ˜ å°„å­—å…¸
        
        Returns:
            label2id, id2label
        """
        return self.label2id, self.id2label


def test_data_loader():
    """
    æµ‹è¯•æ•°æ®åŠ è½½å™¨åŠŸèƒ½
    """
    from config import Config
    
    print("ğŸ§ª æµ‹è¯•æ•°æ®åŠ è½½å™¨...")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    data_loader = NERDataLoader(Config)
    
    # è·å–æ ‡ç­¾æ˜ å°„
    label2id, id2label = data_loader.get_label_mappings()
    print(f"ğŸ“Š æ ‡ç­¾æ•°é‡: {len(label2id)}")
    print(f"ğŸ·ï¸  æ ‡ç­¾æ˜ å°„: {label2id}")
    
    # è·å–æ•°æ®åŠ è½½å™¨
    train_dataloader, valid_dataloader = data_loader.get_dataloaders()
    
    # æµ‹è¯•è®­ç»ƒæ•°æ®
    print(f"ğŸ“ˆ è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_dataloader)}")
    print(f"ğŸ“Š éªŒè¯æ‰¹æ¬¡æ•°: {len(valid_dataloader)}")
    
    # è·å–ä¸€ä¸ªbatchæµ‹è¯•
    for batch in train_dataloader:
        print(f"ğŸ” Input IDså½¢çŠ¶: {batch['input_ids'].shape}")
        print(f"ğŸ¯ Attention Maskå½¢çŠ¶: {batch['attention_mask'].shape}")
        print(f"ğŸ“‹ Labelså½¢çŠ¶: {batch['labels'].shape}")
        print(f"ğŸ“ ç¬¬ä¸€ä¸ªæ ·æœ¬tokens: {batch['tokens'][0]}")
        print(f"ğŸ·ï¸  ç¬¬ä¸€ä¸ªæ ·æœ¬labels: {batch['original_labels'][0]}")
        break
    
    print("âœ… æ•°æ®åŠ è½½å™¨æµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    test_data_loader()
