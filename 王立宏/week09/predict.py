# coding: utf-8

'''
predict.pyï¼šæ¨¡å‹æ•ˆæœæµ‹è¯•
'''

import torch
import json
import os
from typing import List, Tuple, Dict, Any, Optional
import argparse

from config import Config
from device_utils import set_device
from loader import NERDataLoader
from model import create_model
from transformers import BertTokenizerFast


class NERPredictor:
    """
    NERé¢„æµ‹å™¨
    ç”¨äºå¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œå‘½åå®ä½“è¯†åˆ«é¢„æµ‹
    """
    
    def __init__(self, model_path: str, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–é¢„æµ‹å™¨
        
        Args:
            model_path: è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„
            config: é…ç½®å­—å…¸
        """
        self.config = config
        self.device = set_device()
        
        # åŠ è½½æ ‡ç­¾æ˜ å°„
        self.data_loader = NERDataLoader(config)
        self.label2id, self.id2label = self.data_loader.get_label_mappings()
        
        # åŠ è½½åˆ†è¯å™¨
        self.tokenizer = BertTokenizerFast.from_pretrained(config['bert_path'])
        
        # åŠ è½½æ¨¡å‹
        self.model = create_model(config)
        self.model.to(self.device)
        self.load_model(model_path)
        
        # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
        self.model.eval()
        
        print(f"âœ… é¢„æµ‹å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ·ï¸  æ ‡ç­¾æ•°é‡: {len(self.id2label)}")
        print(f"ğŸ“‹ æ ‡ç­¾åˆ—è¡¨: {list(self.id2label.values())}")
    
    def load_model(self, model_path: str):
        """
        åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
        
        Args:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
        """
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        
        checkpoint = torch.load(model_path, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        
        print(f"ğŸ“¦ æ¨¡å‹å·²ä» {model_path} åŠ è½½")
        print(f"ğŸ“Š è®­ç»ƒä¿¡æ¯ - Epoch: {checkpoint['epoch']}, Step: {checkpoint['global_step']}")
        
        if 'metrics' in checkpoint:
            metrics = checkpoint['metrics']
            print(f"ğŸ¯ æ¨¡å‹æ€§èƒ½ - F1: {metrics.get('overall_f1', 0):.4f}, "
                  f"Precision: {metrics.get('overall_precision', 0):.4f}, "
                  f"Recall: {metrics.get('overall_recall', 0):.4f}")
    
    def preprocess_text(self, text: str) -> Tuple[List[str], Dict[str, torch.Tensor]]:
        """
        é¢„å¤„ç†è¾“å…¥æ–‡æœ¬
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            tokens: åˆ†è¯åçš„tokenåˆ—è¡¨
            encoded: ç¼–ç åçš„tensorå­—å…¸
        """
        # å¯¹äºä¸­æ–‡ï¼ŒæŒ‰å­—ç¬¦åˆ†å‰²
        tokens = list(text)
        
        # ä½¿ç”¨BERTåˆ†è¯å™¨ç¼–ç 
        encoded = self.tokenizer(
            tokens,
            is_split_into_words=True,
            max_length=self.config['max_length'],
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        return tokens, encoded
    
    def predict_single(self, text: str) -> Dict[str, Any]:
        """
        å¯¹å•ä¸ªæ–‡æœ¬è¿›è¡Œé¢„æµ‹
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            é¢„æµ‹ç»“æœå­—å…¸
        """
        # é¢„å¤„ç†
        tokens, encoded = self.preprocess_text(text)
        
        # å°†æ•°æ®ç§»åˆ°è®¾å¤‡
        input_ids = encoded['input_ids'].to(self.device)
        attention_mask = encoded['attention_mask'].to(self.device)
        token_type_ids = encoded['token_type_ids'].to(self.device)
        
        # æ¨¡å‹é¢„æµ‹
        with torch.no_grad():
            outputs = self.model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                token_type_ids=token_type_ids
            )
            
            # è·å–é¢„æµ‹æ ‡ç­¾
            predictions = self.model.decode(outputs['emissions'], outputs['mask'])
        
        # å¤„ç†é¢„æµ‹ç»“æœ
        predicted_labels = predictions[0]  # å–ç¬¬ä¸€ä¸ªï¼ˆä¹Ÿæ˜¯å”¯ä¸€ä¸€ä¸ªï¼‰æ ·æœ¬
        
        # å°†é¢„æµ‹ç»“æœä¸åŸå§‹tokenså¯¹é½
        word_ids = encoded.word_ids()
        aligned_predictions = []
        aligned_tokens = []
        
        previous_word_idx = None
        for i, word_idx in enumerate(word_ids):
            if word_idx is None:
                continue  # è·³è¿‡ç‰¹æ®Štoken
            elif word_idx != previous_word_idx:
                if word_idx < len(predicted_labels):
                    label_id = predicted_labels[word_idx]
                    label_name = self.id2label[label_id]
                    aligned_predictions.append(label_name)
                    aligned_tokens.append(tokens[word_idx])
                    previous_word_idx = word_idx
        
        # æå–å‘½åå®ä½“
        entities = self._extract_entities(aligned_tokens, aligned_predictions)
        
        return {
            'text': text,
            'tokens': aligned_tokens,
            'predictions': aligned_predictions,
            'entities': entities
        }
    
    def _extract_entities(self, tokens: List[str], labels: List[str]) -> List[Dict[str, Any]]:
        """
        ä»é¢„æµ‹æ ‡ç­¾ä¸­æå–å‘½åå®ä½“
        
        Args:
            tokens: tokenåˆ—è¡¨
            labels: å¯¹åº”çš„æ ‡ç­¾åˆ—è¡¨
            
        Returns:
            æå–çš„å®ä½“åˆ—è¡¨
        """
        entities = []
        current_entity = None
        
        for i, (token, label) in enumerate(zip(tokens, labels)):
            if label.startswith('B-'):
                # å¼€å§‹æ–°çš„å®ä½“
                if current_entity:
                    entities.append(current_entity)
                
                entity_type = label[2:]  # å»æ‰'B-'
                current_entity = {
                    'text': token,
                    'label': entity_type,
                    'start': i,
                    'end': i,
                    'tokens': [token],
                    'type': entity_type
                }
            
            elif label.startswith('I-'):
                # ç»§ç»­å½“å‰å®ä½“
                entity_type = label[2:]  # å»æ‰'I-'
                if current_entity and current_entity['type'] == entity_type:
                    # ç»§ç»­å½“å‰å®ä½“
                    current_entity['text'] += token
                    current_entity['end'] = i
                    current_entity['tokens'].append(token)
                else:
                    # å¼€å§‹æ–°å®ä½“ï¼ˆé”™è¯¯çš„æ ‡ç­¾åºåˆ—ï¼Œä½†æˆ‘ä»¬è¿˜æ˜¯å¼€å§‹æ–°å®ä½“ï¼‰
                    if current_entity:
                        entities.append(current_entity)
                    current_entity = {
                        'text': token,
                        'label': entity_type,
                        'start': i,
                        'end': i,
                        'tokens': [token],
                        'type': entity_type
                    }
            
            else:  # Oæ ‡ç­¾
                # ç»“æŸå½“å‰å®ä½“
                if current_entity:
                    entities.append(current_entity)
                    current_entity = None
        
        # å¤„ç†æœ€åä¸€ä¸ªå®ä½“
        if current_entity:
            entities.append(current_entity)
        
        return entities
    
    def predict_batch(self, texts: List[str]) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡é¢„æµ‹
        
        Args:
            texts: è¾“å…¥æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            é¢„æµ‹ç»“æœåˆ—è¡¨
        """
        results = []
        
        print(f"ğŸ”„ å¼€å§‹æ‰¹é‡é¢„æµ‹ {len(texts)} ä¸ªæ–‡æœ¬...")
        
        for i, text in enumerate(texts):
            result = self.predict_single(text)
            results.append(result)
            
            # æ‰“å°è¿›åº¦
            if (i + 1) % 10 == 0 or i == len(texts) - 1:
                print(f"ğŸ“Š å·²å¤„ç† {i + 1}/{len(texts)} ä¸ªæ–‡æœ¬")
        
        return results
    
    def print_result(self, result: Dict[str, Any]):
        """
        æ‰“å°é¢„æµ‹ç»“æœ
        
        Args:
            result: é¢„æµ‹ç»“æœå­—å…¸
        """
        print(f"\nğŸ“ åŸå§‹æ–‡æœ¬: {result['text']}")
        print(f"ğŸ·ï¸  æ ‡ç­¾åºåˆ—: {' '.join(result['predictions'])}")
        
        if result['entities']:
            print(f"ğŸ¯ è¯†åˆ«çš„å®ä½“ ({len(result['entities'])}ä¸ª):")
            for i, entity in enumerate(result['entities'], 1):
                print(f"  {i}. {entity['text']} [{entity['type']}] "
                      f"(ä½ç½®: {entity['start']}-{entity['end']})")
        else:
            print("ğŸ¯ æœªè¯†åˆ«åˆ°å®ä½“")
    
    def save_results(self, results: List[Dict[str, Any]], output_path: str):
        """
        ä¿å­˜é¢„æµ‹ç»“æœ
        
        Args:
            results: é¢„æµ‹ç»“æœåˆ—è¡¨
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        # å‡†å¤‡ä¿å­˜çš„æ•°æ®
        save_data = []
        for result in results:
            save_data.append({
                'text': result['text'],
                'entities': result['entities'],
                'tokens': result['tokens'],
                'predictions': result['predictions']
            })
        
        # ä¿å­˜ä¸ºJSONæ ¼å¼
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
    
    def interactive_predict(self):
        """
        äº¤äº’å¼é¢„æµ‹
        """
        print("\n" + "="*60)
        print("ğŸ¯ NER äº¤äº’å¼é¢„æµ‹")
        print("="*60)
        print("è¾“å…¥æ–‡æœ¬è¿›è¡Œå‘½åå®ä½“è¯†åˆ«ï¼Œè¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
        print("-"*60)
        
        while True:
            try:
                text = input("\nğŸ“ è¯·è¾“å…¥æ–‡æœ¬: ").strip()
                
                if text.lower() in ['quit', 'exit', 'q', 'é€€å‡º']:
                    print("ğŸ‘‹ å†è§ï¼")
                    break
                
                if not text:
                    print("âš ï¸  è¯·è¾“å…¥æœ‰æ•ˆæ–‡æœ¬")
                    continue
                
                # é¢„æµ‹
                result = self.predict_single(text)
                
                # æ˜¾ç¤ºç»“æœ
                self.print_result(result)
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ å†è§ï¼")
                break
            except Exception as e:
                print(f"âŒ é¢„æµ‹å‡ºé”™: {e}")


def test_predictor():
    """
    æµ‹è¯•é¢„æµ‹å™¨åŠŸèƒ½
    """
    print("ğŸ§ª æµ‹è¯•NERé¢„æµ‹å™¨...")
    
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
    model_path = os.path.join(Config['model_path'], 'best_model.pt')
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        print("è¯·å…ˆè®­ç»ƒæ¨¡å‹æˆ–æä¾›æ­£ç¡®çš„æ¨¡å‹è·¯å¾„")
        return
    
    # åˆ›å»ºé¢„æµ‹å™¨
    predictor = NERPredictor(model_path, Config)
    
    # æµ‹è¯•æ–‡æœ¬
    test_texts = [
        "å¼ ä¸‰åœ¨åŒ—äº¬çš„æ¸…åå¤§å­¦å·¥ä½œ",
        "ä¸­å›½æ”¿åºœä»£è¡¨å›¢äºæ˜¨å¤©è®¿é—®äº†ç¾å›½çº½çº¦",
        "æå››æ˜¯ä¸€å®¶ç§‘æŠ€å…¬å¸çš„åˆ›å§‹äºº"
    ]
    
    print(f"\nğŸ“‹ æµ‹è¯•æ–‡æœ¬ ({len(test_texts)}ä¸ª):")
    for i, text in enumerate(test_texts, 1):
        print(f"{i}. {text}")
    
    # æ‰¹é‡é¢„æµ‹
    results = predictor.predict_batch(test_texts)
    
    # æ˜¾ç¤ºç»“æœ
    print(f"\nğŸ¯ é¢„æµ‹ç»“æœ:")
    for i, result in enumerate(results, 1):
        predictor.print_result(result)
        print()
    
    # ä¿å­˜ç»“æœ
    output_path = os.path.join(Config['model_path'], 'test_predictions.json')
    predictor.save_results(results, output_path)
    
    print("âœ… é¢„æµ‹å™¨æµ‹è¯•å®Œæˆï¼")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='BERT+CRF NERæ¨¡å‹é¢„æµ‹')
    
    parser.add_argument('--model_path', type=str, 
                       default=os.path.join(Config['model_path'], 'best_model.pt'),
                       help='æ¨¡å‹æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--text', type=str, help='è¦é¢„æµ‹çš„æ–‡æœ¬')
    parser.add_argument('--input_file', type=str, help='è¾“å…¥æ–‡æœ¬æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output_file', type=str, help='è¾“å‡ºç»“æœæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--interactive', action='store_true', help='äº¤äº’å¼é¢„æµ‹æ¨¡å¼')
    parser.add_argument('--test', action='store_true', help='æµ‹è¯•é¢„æµ‹å™¨åŠŸèƒ½')
    
    args = parser.parse_args()
    
    # æµ‹è¯•æ¨¡å¼
    if args.test:
        test_predictor()
        return
    
    # åˆ›å»ºé¢„æµ‹å™¨
    try:
        predictor = NERPredictor(args.model_path, Config)
    except FileNotFoundError as e:
        print(f"âŒ {e}")
        return
    
    # äº¤äº’å¼é¢„æµ‹
    if args.interactive:
        predictor.interactive_predict()
        return
    
    # å•æ–‡æœ¬é¢„æµ‹
    if args.text:
        result = predictor.predict_single(args.text)
        predictor.print_result(result)
        
        if args.output_file:
            predictor.save_results([result], args.output_file)
        return
    
    # æ–‡ä»¶æ‰¹é‡é¢„æµ‹
    if args.input_file:
        if not os.path.exists(args.input_file):
            print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input_file}")
            return
        
        # è¯»å–æ–‡æœ¬æ–‡ä»¶
        with open(args.input_file, 'r', encoding='utf-8') as f:
            texts = [line.strip() for line in f if line.strip()]
        
        print(f"ğŸ“– ä»æ–‡ä»¶è¯»å–äº† {len(texts)} ä¸ªæ–‡æœ¬")
        
        # æ‰¹é‡é¢„æµ‹
        results = predictor.predict_batch(texts)
        
        # ä¿å­˜ç»“æœ
        if args.output_file:
            predictor.save_results(results, args.output_file)
        else:
            # æ˜¾ç¤ºå‰å‡ ä¸ªç»“æœ
            for i, result in enumerate(results[:3], 1):
                predictor.print_result(result)
        
        return
    
    # é»˜è®¤äº¤äº’æ¨¡å¼
    print("æœªæŒ‡å®šé¢„æµ‹å†…å®¹ï¼Œè¿›å…¥äº¤äº’æ¨¡å¼...")
    predictor.interactive_predict()


if __name__ == "__main__":
    main()

