# coding: utf-8

'''
config.pyï¼šè¾“å…¥æ¨¡å‹é…ç½®å‚æ•°ï¼Œå¦‚å­¦ä¹ ç‡ã€æ¨¡å‹ä¿å­˜ä½ç½®ç­‰
'''
#!/usr/bin/env python3
"""
è®¾å¤‡å·¥å…·ç±» - è‡ªåŠ¨æ£€æµ‹å’Œé€‰æ‹©æœ€ä½³è®¾å¤‡
æ”¯æŒGPUã€NPUã€MPSã€CPU
"""

import torch
import platform
import warnings

def auto_select_device():
    """
    è‡ªåŠ¨é€‰æ‹©æœ€ä½³è®¾å¤‡
    
    ä¼˜å…ˆçº§: NPU > GPU > MPS > CPU
    
    Returns:
        str: è®¾å¤‡ç±»å‹ ('cuda', 'npu', 'mps', 'cpu')
    """
    
    # 1. æ£€æŸ¥NPU (åä¸ºæ˜‡è…¾)
    if _check_npu_available():
        return 'npu'
    
    # 2. æ£€æŸ¥CUDA (NVIDIA GPU)
    elif _check_cuda_available():
        return 'cuda'
    
    # 3. æ£€æŸ¥MPS (Apple Silicon GPU)
    elif _check_mps_available():
        return 'mps'
    
    # 4. é»˜è®¤ä½¿ç”¨CPU
    else:
        return 'cpu'

def _check_npu_available():
    """æ£€æŸ¥NPUæ˜¯å¦å¯ç”¨"""
    try:
        import torch_npu
        if torch_npu.npu.is_available():
            device_count = torch_npu.npu.device_count()
            if device_count > 0:
                print(f"âœ… æ£€æµ‹åˆ° {device_count} ä¸ªNPUè®¾å¤‡")
                return True
        return False
    except ImportError:
        return False
    except Exception as e:
        warnings.warn(f"æ£€æŸ¥NPUæ—¶å‡ºé”™: {e}")
        return False

def _check_cuda_available():
    """æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨"""
    try:
        if torch.cuda.is_available():
            device_count = torch.cuda.device_count()
            if device_count > 0:
                # è·å–GPUåç§°
                gpu_name = torch.cuda.get_device_name(0)
                print(f"âœ… æ£€æµ‹åˆ° {device_count} ä¸ªCUDAè®¾å¤‡: {gpu_name}")
                return True
        return False
    except Exception as e:
        warnings.warn(f"æ£€æŸ¥CUDAæ—¶å‡ºé”™: {e}")
        return False

def _check_mps_available():
    """æ£€æŸ¥MPSæ˜¯å¦å¯ç”¨ï¼ˆApple Siliconï¼‰"""
    try:
        if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            print("âœ… æ£€æµ‹åˆ°MPSè®¾å¤‡ï¼ˆApple Siliconï¼‰")
            return True
        return False
    except Exception as e:
        warnings.warn(f"æ£€æŸ¥MPSæ—¶å‡ºé”™: {e}")
        return False

def get_device_info():
    """è·å–è®¾å¤‡è¯¦ç»†ä¿¡æ¯"""
    device = auto_select_device()
    info = {
        'selected_device': device,
        'device_count': 0,
        'device_name': None,
        'platform': platform.system(),
        'python_version': platform.python_version(),
        'torch_version': torch.__version__
    }
    
    if device == 'cuda':
        info['device_count'] = torch.cuda.device_count()
        info['device_name'] = torch.cuda.get_device_name(0)
        info['memory_total'] = torch.cuda.get_device_properties(0).total_memory
        info['memory_allocated'] = torch.cuda.memory_allocated()
        
    elif device == 'npu':
        try:
            import torch_npu
            info['device_count'] = torch_npu.npu.device_count()
            # NPUè®¾å¤‡ä¿¡æ¯å¯èƒ½éœ€è¦torch_npuæä¾›çš„ç‰¹å®šAPI
            info['device_name'] = 'NPU'
        except ImportError:
            pass
            
    elif device == 'mps':
        info['device_name'] = 'Apple Silicon GPU'
        
    else:
        info['device_name'] = 'CPU'
    
    return info

def print_device_info():
    """æ‰“å°è®¾å¤‡ä¿¡æ¯"""
    info = get_device_info()
    
    print("=" * 60)
    print("ğŸ–¥ï¸  è®¾å¤‡ä¿¡æ¯")
    print("=" * 60)
    print(f"ğŸ“± å¹³å°: {info['platform']}")
    print(f"ğŸ Pythonç‰ˆæœ¬: {info['python_version']}")
    print(f"ğŸ”¥ PyTorchç‰ˆæœ¬: {info['torch_version']}")
    print(f"ğŸ¯ é€‰æ‹©è®¾å¤‡: {info['selected_device'].upper()}")
    print(f"ğŸ”§ è®¾å¤‡åç§°: {info['device_name']}")
    print(f"ğŸ“Š è®¾å¤‡æ•°é‡: {info['device_count']}")
    
    if 'memory_total' in info:
        memory_gb = info['memory_total'] / (1024**3)
        allocated_gb = info['memory_allocated'] / (1024**3)
        print(f"ğŸ’¾ æ€»å†…å­˜: {memory_gb:.2f} GB")
        print(f"ğŸ’¾ å·²ç”¨å†…å­˜: {allocated_gb:.2f} GB")
    
    print("=" * 60)

def set_device(device=None):
    """
    è®¾ç½®å¹¶è¿”å›torchè®¾å¤‡
    
    Args:
        device: æŒ‡å®šè®¾å¤‡ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨é€‰æ‹©
        
    Returns:
        torch.device: torchè®¾å¤‡å¯¹è±¡
    """
    if device is None:
        device = auto_select_device()
    
    if device == 'cuda':
        torch_device = torch.device('cuda')
    elif device == 'npu':
        torch_device = torch.device('npu')
    elif device == 'mps':
        torch_device = torch.device('mps')
    else:
        torch_device = torch.device('cpu')
    
    return torch_device

def optimize_for_device(device=None):
    """
    é’ˆå¯¹ä¸åŒè®¾å¤‡è¿›è¡Œä¼˜åŒ–è®¾ç½®
    
    Args:
        device: è®¾å¤‡ç±»å‹
    """
    if device is None:
        device = auto_select_device()
    
    # CUDAä¼˜åŒ–
    if device == 'cuda':
        # å¯ç”¨cudnn benchmark
        torch.backends.cudnn.benchmark = True
        # å¯ç”¨cudnn deterministicï¼ˆå¦‚æœéœ€è¦å¯é‡ç°ç»“æœï¼‰
        # torch.backends.cudnn.deterministic = True
        print("ğŸš€ å¯ç”¨CUDAä¼˜åŒ–è®¾ç½®")
    
    # MPSä¼˜åŒ–
    elif device == 'mps':
        # MPSçš„ä¸€äº›ä¼˜åŒ–è®¾ç½®
        print("ğŸš€ å¯ç”¨MPSä¼˜åŒ–è®¾ç½®")
    
    # NPUä¼˜åŒ–
    elif device == 'npu':
        try:
            import torch_npu
            # NPUç‰¹å®šä¼˜åŒ–è®¾ç½®
            torch_npu.npu.set_compile_mode(jit_compile=False)
            print("ğŸš€ å¯ç”¨NPUä¼˜åŒ–è®¾ç½®")
        except ImportError:
            pass
    
    # CPUä¼˜åŒ–
    else:
        # è®¾ç½®CPUçº¿ç¨‹æ•°
        torch.set_num_threads(4)
        print("ğŸš€ å¯ç”¨CPUä¼˜åŒ–è®¾ç½®")

# æµ‹è¯•å‡½æ•°
def test_device():
    """æµ‹è¯•è®¾å¤‡åŠŸèƒ½"""
    device = set_device()
    print(f"ğŸ§ª æµ‹è¯•è®¾å¤‡: {device}")
    
    # åˆ›å»ºæµ‹è¯•å¼ é‡
    try:
        x = torch.randn(1000, 1000).to(device)
        y = torch.randn(1000, 1000).to(device)
        z = torch.mm(x, y)
        print(f"âœ… è®¾å¤‡æµ‹è¯•æˆåŠŸï¼Œå¼ é‡å½¢çŠ¶: {z.shape}")
        return True
    except Exception as e:
        print(f"âŒ è®¾å¤‡æµ‹è¯•å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    # æ‰“å°è®¾å¤‡ä¿¡æ¯
    print_device_info()
    
    # æµ‹è¯•è®¾å¤‡
    test_device()
    
    # ä¼˜åŒ–è®¾ç½®
    device = auto_select_device()
    optimize_for_device(device)