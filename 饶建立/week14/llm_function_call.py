import json
import openai  # æˆ–å…¶ä»–å¤§æ¨¡å‹API
from typing import Dict, Any


# æ¨¡æ‹Ÿä¸€ä¸ªç®€å•çš„å‡½æ•°æ³¨å†Œå™¨
class FunctionManager:
    def __init__(self):
        self.functions = {}

    def register(self, func):
        """æ³¨å†Œå‡½æ•°"""
        self.functions[func.__name__] = func
        return func

    def get_tools_schema(self):
        """ç”Ÿæˆå·¥å…·schemaï¼ˆOpenAIæ ¼å¼ï¼‰"""
        tools = []

        for name, func in self.functions.items():
            # è¿™é‡Œç®€åŒ–äº†ï¼Œå®é™…åº”è¯¥è§£æå‡½æ•°å‚æ•°å’Œæ–‡æ¡£
            if name == "get_weather":
                schema = {
                    "type": "function",
                    "function": {
                        "name": "get_weather",
                        "description": "è·å–åŸå¸‚çš„å¤©æ°”ä¿¡æ¯",
                        "parameters": {
                            "type": "object",
                            "properties": {
                                "city": {
                                    "type": "string",
                                    "description": "åŸå¸‚åç§°ï¼Œå¦‚ï¼šåŒ—äº¬ã€ä¸Šæµ·"
                                }
                            },
                            "required": ["city"]
                        }
                    }
                }
                tools.append(schema)

            elif name == "calculator":
                schema = {
                    "type": "function",
                    "function": {
                        "name": "calculator",
                        "description": "è®¡ç®—æ•°å­¦è¡¨è¾¾å¼",
                        "parameters": {
                            "type": "object",
                            "properties": {
                                "expression": {
                                    "type": "string",
                                    "description": "æ•°å­¦è¡¨è¾¾å¼ï¼Œå¦‚ï¼š2+3*4"
                                }
                            },
                            "required": ["expression"]
                        }
                    }
                }
                tools.append(schema)

        return tools

    def execute(self, function_name: str, arguments: Dict[str, Any]) -> Any:
        """æ‰§è¡Œå‡½æ•°"""
        return self.functions[function_name](**arguments)


# åˆ›å»ºå‡½æ•°ç®¡ç†å™¨
fm = FunctionManager()


# æ³¨å†Œå·¥å…·å‡½æ•°
@fm.register
def get_weather(city: str) -> str:
    """è·å–å¤©æ°”ä¿¡æ¯"""
    # è¿™é‡Œæ¨¡æ‹Ÿè°ƒç”¨å¤©æ°”API
    weather_data = {
        "åŒ—äº¬": "æ™´å¤©ï¼Œ25Â°Cï¼Œç©ºæ°”è´¨é‡è‰¯",
        "ä¸Šæµ·": "å¤šäº‘ï¼Œ28Â°Cï¼Œç©ºæ°”è´¨é‡ä¼˜",
        "å¹¿å·": "é˜µé›¨ï¼Œ30Â°Cï¼Œç©ºæ°”è´¨é‡è‰¯"
    }
    return weather_data.get(city, "æœªçŸ¥åŸå¸‚")


@fm.register
def calculator(expression: str) -> str:
    """è®¡ç®—æ•°å­¦è¡¨è¾¾å¼"""
    try:
        result = eval(expression)  # æ³¨æ„ï¼šç”Ÿäº§ç¯å¢ƒè¦ç”¨æ›´å®‰å…¨çš„æ–¹æ³•
        return f"{expression} = {result}"
    except:
        return "è®¡ç®—å¤±è´¥"


# æ¨¡æ‹Ÿå¤§æ¨¡å‹APIè°ƒç”¨
class MockLLM:
    """æ¨¡æ‹Ÿå¤§æ¨¡å‹API"""

    def chat_completion(self, messages, tools=None):
        """æ¨¡æ‹Ÿå¤§æ¨¡å‹çš„å“åº”"""

        user_input = messages[-1]["content"]

        # æ¨¡æ‹Ÿå¤§æ¨¡å‹æ€è€ƒï¼šæ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·ï¼Ÿ
        if "å¤©æ°”" in user_input or "temperature" in user_input.lower():
            # å¤§æ¨¡å‹å†³å®šè°ƒç”¨ get_weather å‡½æ•°
            city = "åŒ—äº¬"  # ç®€å•æå–ï¼Œå®é™…å¤§æ¨¡å‹ä¼šæ™ºèƒ½æå–
            if "ä¸Šæµ·" in user_input:
                city = "ä¸Šæµ·"
            elif "å¹¿å·" in user_input:
                city = "å¹¿å·"

            return {
                "choices": [{
                    "message": {
                        "content": None,
                        "tool_calls": [{
                            "id": "call_123",
                            "type": "function",
                            "function": {
                                "name": "get_weather",
                                "arguments": json.dumps({"city": city})
                            }
                        }]
                    }
                }]
            }

        elif "è®¡ç®—" in user_input or any(op in user_input for op in ["+", "-", "*", "/"]):
            # å¤§æ¨¡å‹å†³å®šè°ƒç”¨ calculator å‡½æ•°
            import re
            match = re.search(r'(\d+[\+\-\*/]\d+)', user_input)
            expression = match.group(1) if match else "2+2"

            return {
                "choices": [{
                    "message": {
                        "content": None,
                        "tool_calls": [{
                            "id": "call_456",
                            "type": "function",
                            "function": {
                                "name": "calculator",
                                "arguments": json.dumps({"expression": expression})
                            }
                        }]
                    }
                }]
            }

        # ä¸éœ€è¦å·¥å…·è°ƒç”¨ï¼Œç›´æ¥å›å¤
        return {
            "choices": [{
                "message": {
                    "content": f"æˆ‘æ”¶åˆ°äº†ä½ çš„æ¶ˆæ¯ï¼š'{user_input}'",
                    "tool_calls": None
                }
            }]
        }


# ä¸»äº¤äº’æµç¨‹
def chat_with_tools(user_input: str):
    """å®Œæ•´çš„èŠå¤©æµç¨‹"""

    llm = MockLLM()
    conversation_history = []

    print(f"\nğŸ‘¤ ç”¨æˆ·: {user_input}")

    # 1. å‡†å¤‡æ¶ˆæ¯
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ï¼Œå¯ä»¥è°ƒç”¨å·¥å…·æ¥å¸®åŠ©ç”¨æˆ·ã€‚"},
        {"role": "user", "content": user_input}
    ]

    # 2. ç¬¬ä¸€æ¬¡è°ƒç”¨å¤§æ¨¡å‹ï¼ˆè®©æ¨¡å‹å†³å®šæ˜¯å¦è°ƒç”¨å·¥å…·ï¼‰
    print("ğŸ¤– å¤§æ¨¡å‹æ€è€ƒä¸­...")
    response = llm.chat_completion(
        messages=messages,
        tools=fm.get_tools_schema()  # å‘Šè¯‰æ¨¡å‹æœ‰å“ªäº›å·¥å…·å¯ç”¨
    )

    message = response["choices"][0]["message"]

    # 3. æ£€æŸ¥æ˜¯å¦è°ƒç”¨äº†å·¥å…·
    if message.get("tool_calls"):
        print("ğŸ› ï¸  å¤§æ¨¡å‹å†³å®šè°ƒç”¨å·¥å…·...")

        for tool_call in message["tool_calls"]:
            func_name = tool_call["function"]["name"]
            arguments = json.loads(tool_call["function"]["arguments"])

            print(f"  è°ƒç”¨å·¥å…·: {func_name}")
            print(f"  å‚æ•°: {arguments}")

            # 4. æ‰§è¡Œå·¥å…·å‡½æ•°
            try:
                tool_result = fm.execute(func_name, arguments)
                print(f"  å·¥å…·æ‰§è¡Œç»“æœ: {tool_result}")

                # 5. æŠŠç»“æœå‘é€å›å¤§æ¨¡å‹
                messages.append(message)  # æ·»åŠ æ¨¡å‹çš„å·¥å…·è°ƒç”¨è¯·æ±‚
                messages.append({
                    "role": "tool",
                    "tool_call_id": tool_call["id"],
                    "content": str(tool_result)
                })

                # 6. ç¬¬äºŒæ¬¡è°ƒç”¨å¤§æ¨¡å‹ï¼ˆè®©å®ƒåŸºäºå·¥å…·ç»“æœç”Ÿæˆå›å¤ï¼‰
                print("ğŸ¤– å¤§æ¨¡å‹åŸºäºå·¥å…·ç»“æœç”Ÿæˆå›ç­”...")
                final_response = llm.chat_completion(messages=messages)
                final_message = final_response["choices"][0]["message"]

                print(f"ğŸ’¬ åŠ©æ‰‹: {final_message['content']}")
                return final_message['content']

            except Exception as e:
                print(f"âŒ å·¥å…·æ‰§è¡Œå¤±è´¥: {e}")
                return f"å·¥å…·æ‰§è¡Œå¤±è´¥: {e}"

    else:
        # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç›´æ¥å›å¤
        print(f"ğŸ’¬ åŠ©æ‰‹: {message['content']}")
        return message['content']


# è¿è¡Œç¤ºä¾‹
def main():
    print("=== å¤§æ¨¡å‹ä¸ Function Call äº¤äº’æ¼”ç¤º ===\n")

    # æµ‹è¯•ä¸åŒæƒ…å†µ
    test_cases = [
        "ä»Šå¤©åŒ—äº¬å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ",
        "è®¡ç®—ä¸€ä¸‹ 25+37*2 ç­‰äºå¤šå°‘",
        "ä½ å¥½ï¼Œæˆ‘æ˜¯æ–°ç”¨æˆ·",
        "ä¸Šæµ·å’Œå¹¿å·çš„å¤©æ°”å¯¹æ¯”ä¸€ä¸‹",
        "å¸®æˆ‘ç®—ä¸€ä¸‹ (100-25)/3 çš„ç»“æœ"
    ]

    for query in test_cases:
        chat_with_tools(query)
        print("-" * 50)


# çœŸå®APIè°ƒç”¨ç¤ºä¾‹ï¼ˆä½¿ç”¨OpenAIï¼‰
def real_openai_example():
    """ä½¿ç”¨çœŸå®OpenAI APIçš„ç¤ºä¾‹"""

    # æ³¨æ„ï¼šéœ€è¦å®‰è£… openai åº“å¹¶è®¾ç½® API_KEY
    import os

    # è®¾ç½®APIå¯†é’¥
    # os.environ["OPENAI_API_KEY"] = "your-api-key"

    # åˆ›å»ºOpenAIå®¢æˆ·ç«¯
    client = openai.OpenAI()

    # 1. ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼šè®©æ¨¡å‹å†³å®šæ˜¯å¦ä½¿ç”¨å·¥å…·
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "user", "content": "ä»Šå¤©åŒ—äº¬å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"}
        ],
        tools=fm.get_tools_schema(),  # æä¾›å·¥å…·å®šä¹‰
        tool_choice="auto"  # è®©æ¨¡å‹è‡ªåŠ¨å†³å®š
    )

    message = response.choices[0].message

    # 2. å¦‚æœæ¨¡å‹è°ƒç”¨äº†å·¥å…·
    if message.tool_calls:
        for tool_call in message.tool_calls:
            func_name = tool_call.function.name
            arguments = json.loads(tool_call.function.arguments)

            # æ‰§è¡Œå·¥å…·
            result = fm.execute(func_name, arguments)

            # 3. ç¬¬äºŒæ¬¡è°ƒç”¨ï¼šæŠŠå·¥å…·ç»“æœç»™æ¨¡å‹
            second_response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "user", "content": "ä»Šå¤©åŒ—äº¬å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"},
                    message,  # æ¨¡å‹çš„å·¥å…·è°ƒç”¨è¯·æ±‚
                    {
                        "role": "tool",
                        "tool_call_id": tool_call.id,
                        "content": str(result)
                    }
                ]
            )

            print(f"æœ€ç»ˆå›ç­”: {second_response.choices[0].message.content}")


# ç®€åŒ–ç‰ˆçš„äº¤äº’æµç¨‹å›¾
def visualize_interaction():
    """å¯è§†åŒ–äº¤äº’æµç¨‹"""
    print("\n" + "=" * 60)
    print("å¤§æ¨¡å‹ä¸ Function Call äº¤äº’æµç¨‹ï¼š")
    print("=" * 60)

    steps = [
        ("1. ç”¨æˆ·è¾“å…¥", "ğŸ‘¤: 'ä»Šå¤©åŒ—äº¬å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ'"),
        ("2. å¤§æ¨¡å‹åˆ†æ", "ğŸ¤–: 'ç”¨æˆ·é—®å¤©æ°”ï¼Œæˆ‘éœ€è¦è°ƒç”¨å¤©æ°”å·¥å…·'"),
        ("3. è¿”å›å·¥å…·è°ƒç”¨", "ğŸ“¤: {'name': 'get_weather', 'arguments': {'city': 'åŒ—äº¬'}}"),
        ("4. æ‰§è¡Œå‡½æ•°", "âš™ï¸: è°ƒç”¨å¤©æ°”APIè·å–æ•°æ®"),
        ("5. è¿”å›ç»“æœ", "ğŸ“¥: 'åŒ—äº¬ï¼šæ™´å¤©ï¼Œ25Â°C'"),
        ("6. å¤§æ¨¡å‹ç”Ÿæˆå›å¤", "ğŸ¤–: 'æ ¹æ®å¤©æ°”APIï¼Œä»Šå¤©åŒ—äº¬æ™´å¤©ï¼Œ25Â°C...'"),
        ("7. æœ€ç»ˆè¾“å‡º", "ğŸ’¬: 'ä»Šå¤©åŒ—äº¬å¤©æ°”å¾ˆå¥½ï¼Œæ™´å¤©ï¼Œæ¸©åº¦25Â°C...'")
    ]

    for step, desc in steps:
        print(f"{step:20} {desc}")

    print("=" * 60)


# å¤šè½®å¯¹è¯ç¤ºä¾‹
def multi_turn_conversation():
    """å¤šè½®å¯¹è¯ä¸­çš„ Function Call"""

    llm = MockLLM()
    messages = [
        {"role": "system", "content": "ä½ æ˜¯æœ‰ç”¨çš„åŠ©æ‰‹ï¼Œå¯ä»¥è°ƒç”¨å·¥å…·ã€‚"}
    ]

    def process_round(user_input):
        messages.append({"role": "user", "content": user_input})

        # ç¬¬ä¸€æ¬¡è°ƒç”¨
        response = llm.chat_completion(messages, fm.get_tools_schema())
        message = response["choices"][0]["message"]

        if message.get("tool_calls"):
            # å¤„ç†å·¥å…·è°ƒç”¨
            messages.append(message)

            for tool_call in message["tool_calls"]:
                func_name = tool_call["function"]["name"]
                arguments = json.loads(tool_call["function"]["arguments"])

                result = fm.execute(func_name, arguments)

                messages.append({
                    "role": "tool",
                    "tool_call_id": tool_call["id"],
                    "content": str(result)
                })

            # ç¬¬äºŒæ¬¡è°ƒç”¨è·å–æœ€ç»ˆå›å¤
            final_response = llm.chat_completion(messages)
            assistant_msg = final_response["choices"][0]["message"]
            messages.append(assistant_msg)

            return assistant_msg["content"]
        else:
            messages.append(message)
            return message["content"]

    # æ¨¡æ‹Ÿå¯¹è¯
    print("\n=== å¤šè½®å¯¹è¯ç¤ºä¾‹ ===")

    queries = [
        "åŒ—äº¬å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ",
        "é‚£ä¸Šæµ·å‘¢ï¼Ÿ",
        "è®¡ç®—ä¸€ä¸‹ä¸¤åœ°çš„æ¸©å·®"
    ]

    for query in queries:
        print(f"\nğŸ‘¤: {query}")
        response = process_round(query)
        print(f"ğŸ¤–: {response}")


if __name__ == "__main__":
    # è¿è¡Œä¸»æ¼”ç¤º
    main()

    # æ˜¾ç¤ºäº¤äº’æµç¨‹
    visualize_interaction()

    # è¿è¡Œå¤šè½®å¯¹è¯ç¤ºä¾‹
    multi_turn_conversation()